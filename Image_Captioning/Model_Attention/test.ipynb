{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"deeplearning","language":"python","name":"deeplearning"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"test.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"KVegTS3xUe0Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":188},"outputId":"5850eb96-0755-4e27-f657-3e5c48078fec","executionInfo":{"status":"ok","timestamp":1587935382803,"user_tz":300,"elapsed":37304,"user":{"displayName":"Tarun Kateja","photoUrl":"","userId":"06250246655916412153"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/My\\ Drive/Colab\\ Notebooks/IDS576_Deep_Learning/Project_New/Image_Cap_Attn\n","\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/IDS576_Deep_Learning/Project_New/Image_Cap_Attn\n","flickr8k_build_vocab.py  flickr8k_model.py   flickr8k_train.py\t test.ipynb\n","flickr8k_data_loader.py  flickr8k_models     flickr8k_vocab.pkl\n","flickr8k_eval.py\t flickr8k_sample.py  __pycache__\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u6tRZAojUce1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"a72dfda3-9db8-42f4-ee87-6a94c19dac68","executionInfo":{"status":"error","timestamp":1587935780986,"user_tz":300,"elapsed":2415,"user":{"displayName":"Tarun Kateja","photoUrl":"","userId":"06250246655916412153"}}},"source":["import argparse\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","import pickle\n","from flickr8k_data_loader import get_validation_loader \n","from flickr8k_build_vocab import Vocabulary\n","from flickr8k_model import EncoderCNN, DecoderRNNWithAttention\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torchvision import transforms\n","from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n","\n","# Device configuration\n","device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","def main(args):\n","    # Image preprocessing\n","    transform = transforms.Compose([\n","        transforms.Resize((args.image_size, args.image_size)),\n","        transforms.ToTensor(), \n","     #   transforms.Normalize((0.485, 0.456, 0.406), \n","      #                       (0.229, 0.224, 0.225))\n","    ])\n","    \n","    # Load vocabulary wrapper\n","    with open(args.vocab_path, 'rb') as f:\n","        vocab = pickle.load(f)\n","        \n","    # Build data loader\n","    data_loader = get_validation_loader(args.image_dir, args.caption_path, vocab, \n","                             transform, args.batch_size,\n","                             num_workers=args.num_workers)\n","    \n","    # Build models\n","    encoder = EncoderCNN(args.encoded_image_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n","    decoder = DecoderRNNWithAttention(args.embed_size, args.attention_size, args.hidden_size, len(vocab)).eval()\n","    encoder = encoder.to(device)\n","    decoder = decoder.to(device)\n","\n","    # Load the trained model parameters\n","    encoder.load_state_dict(torch.load(args.encoder_path))\n","    decoder.load_state_dict(torch.load(args.decoder_path))\n","    \n","    ground_truth = []\n","    predicted = []\n","    for i, (images, captions) in enumerate(data_loader):\n","        # Set mini-batch dataset\n","        images = images.to(device)\n","        features = encoder(images)\n","        sampled_seq = decoder.sample_beam_search(features, vocab, device)\n","        ground_truth.append(captions[0])\n","        predicted.append(sampled_seq[0])\n","        if i > 10: break\n","            \n","    print(corpus_bleu(ground_truth, predicted, weights=(1, 0, 0, 0)))\n","    print(corpus_bleu(ground_truth, predicted, weights=(0.5, 0.5, 0, 0)))\n","    print(corpus_bleu(ground_truth, predicted, weights=(1.0/3.0, 1.0/3.0, 1.0/3.0, 0)))\n","    print(corpus_bleu(ground_truth, predicted))\n","\n","if __name__ == '__main__':\n","    import sys\n","    sys.argv = ['foo']\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--encoder_path', type=str, default='./flickr8k_models/encoder-24-213.ckpt', help='path for trained encoder')\n","    parser.add_argument('--decoder_path', type=str, default='./flickr8k_models/decoder-24-213.ckpt', help='path for trained decoder')\n","    parser.add_argument('--vocab_path', type=str, default='./flickr8k_vocab.pkl', help='path for vocabulary wrapper')\n","    parser.add_argument('--image_dir', type=str, default='../flickr8k/Flickr_Data/Flickr_Data/Images/', help='directory for resized images')\n","    parser.add_argument('--caption_path', type=str, default='../flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt', help='path for train annotation json file')\n","    parser.add_argument('--image_size', type=int , default=224, help='input image size')\n","\n","    # Model parameters (should be same as paramters in train.py)\n","    parser.add_argument('--embed_size', type=int , default=300, help='dimension of word embedding vectors')\n","    parser.add_argument('--encoded_image_size', type=int , default=14, help='dimension of encoded image')\n","    parser.add_argument('--attention_size', type=int , default=512, help='dimension of attention layers')\n","    parser.add_argument('--hidden_size', type=int , default=512, help='dimension of lstm hidden states')\n","\n","    parser.add_argument('--batch_size', type=int, default=1)\n","    parser.add_argument('--num_workers', type=int, default=4)\n","\n","    args = parser.parse_args()\n","    print(args)\n","    main(args)\n"],"execution_count":2,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-47eac3f5c37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcoco_data_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_validation_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoco_build_vocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoco_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoderRNNWithAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'coco_data_loader'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"9z5Fg5QbWRay","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}